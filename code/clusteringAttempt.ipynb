{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in data, and merge\n",
    "base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "emissions = pd.read_csv(os.path.join(base_dir, \"data\", \"co2_emissions.csv\"))\n",
    "print(emissions.head())\n",
    "\n",
    "methane = pd.read_csv(os.path.join(base_dir, \"data\", \"methane_emissions.csv\"))\n",
    "print(methane.head())\n",
    "\n",
    "forest = pd.read_csv(os.path.join(base_dir, \"data\", \"net_forest.csv\"))\n",
    "print(forest.head())\n",
    "\n",
    "surface = pd.read_csv(os.path.join(base_dir, \"data\", \"surface_temperature.csv\"))\n",
    "print(surface.head())\n",
    "\n",
    "merge = emissions.merge(methane, on=['country_code', \"year\"], how=\"outer\")\n",
    "merge = merge.merge(forest, on=['country_code', \"year\"], how=\"outer\")\n",
    "merge = merge.merge(surface, on=['country_code', \"year\"], how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function performs gonzales clustering, it assumes data points to be numpy vectors\n",
    "def gonazalesCluster(data, numClusters):\n",
    "  #initalize first center to be first point\n",
    "  centroids = [data[0]]\n",
    "  \n",
    "  #set the cluster mapping of all to be cluster 1\n",
    "  cluster = []\n",
    "  for i in range(len(data)):\n",
    "    cluster.append(0)\n",
    "\n",
    "\n",
    "  #for i = 2 to k \n",
    "  for i in range(1, numClusters):\n",
    "    #set m=0 and ci = x1\n",
    "    m=0\n",
    "    ci = data[0]\n",
    "    #for each datapoint\n",
    "    for j in range(len(data)):\n",
    "      #compute distance from point to mapped cluster\n",
    "      dist = np.linalg.norm(data[j]-centroids[cluster[j]])\n",
    "  \n",
    "        #if this distance is greater than m, set m and ci = point\n",
    "      if dist>m:\n",
    "        m= dist\n",
    "        ci = data[j]\n",
    "    centroids.append(ci)\n",
    "    #for each datapoint\n",
    "    for j in range(len(data)):\n",
    "      #if the distance to its current cluster is greater than the distance to the new cluster\n",
    "      if (np.linalg.norm(data[j]-centroids[cluster[j]]) > np.linalg.norm(data[j]-ci)):\n",
    "  \n",
    "        #set the mapped cluster of this point to be the new cluster\n",
    "        cluster[j]=i\n",
    "\n",
    "  #compute 3 center cost max\n",
    "  #andcomputer the 3 means cost\n",
    "  sum = 0\n",
    "  maxCenter = 0\n",
    "  for i in range(len(data)):\n",
    "    dist = np.linalg.norm(data[i] - centroids[cluster[i]])\n",
    "    sum = sum + (dist * dist)\n",
    "    if dist > maxCenter:\n",
    "      maxCenter = dist\n",
    "\n",
    "  meanCost = sum / len(data)\n",
    "  \n",
    "\n",
    "  return centroids, cluster, maxCenter, meanCost\n",
    "\n",
    "#this function performs gonzales clustering, it assumes data points to be numpy vectors\n",
    "def kPlusPlus(data, numClusters):\n",
    "  #set the first cluster to be the first point\n",
    "  centroids = [data[0]]\n",
    "  \n",
    "  #set the cluster mapping of all to be cluster 1\n",
    "  cluster = []\n",
    "  for i in range(len(data)):\n",
    "    cluster.append(0)\n",
    "\n",
    "  #for k = 1 to numClusters:\n",
    "  for k in range(1, numClusters):\n",
    "    #initalize a sum\n",
    "    sum = 0\n",
    "  \n",
    "    #intialize a probability distribution\n",
    "    density = []\n",
    "\n",
    "    #choose ci from x with probability proportional two the squared distance of X and the previous cluster\n",
    "    #iterate through the points\n",
    "    for i in range(len(data)):\n",
    "  \n",
    "      #compute the squared distance to the point and the cluster\n",
    "      dist = np.linalg.norm(data[i] - centroids[cluster[i]])\n",
    "      dist = dist **2\n",
    "  \n",
    "      #add it to the sum\n",
    "      sum = sum+dist\n",
    "  \n",
    "      #append the sum to the probability distirbution\n",
    "      density.append(sum)\n",
    "  \n",
    "    #compute a random number between 0 and sum\n",
    "    rand = np.random.uniform(0, sum)\n",
    "    nextCluster = []\n",
    "    #iterate through the distribution until you hit a point that is larger, then that index becomes the new cluster\n",
    "    for i in range(len(data)):\n",
    "      if density[i] < rand:\n",
    "        continue\n",
    "      nextCluster = data[i]\n",
    "      break\n",
    "\n",
    "    centroids.append(nextCluster)\n",
    "  \n",
    "\n",
    "    #set the new clusters\n",
    "    for j in range(len(data)):\n",
    "      #if the distance to its current cluster is greater than the distance to the new cluster\n",
    "      if (np.linalg.norm(data[j]-centroids[cluster[j]]) > np.linalg.norm(data[j]-nextCluster)):\n",
    "  \n",
    "        #set the mapped cluster of this point to be the new cluster\n",
    "        cluster[j]=k\n",
    "\n",
    "    \n",
    "  \n",
    "  #compute 3 center cost max\n",
    "  #and compute  the 3 means cost\n",
    "  sum = 0\n",
    "  maxCenter = 0\n",
    "  for i in range(len(data)):\n",
    "    dist = np.linalg.norm(data[i] - centroids[cluster[i]])\n",
    "    sum = sum + (dist * dist)\n",
    "    if dist > maxCenter:\n",
    "      maxCenter = dist\n",
    "\n",
    "  meanCost = sum / len(data)\n",
    "    \n",
    "\n",
    "  return centroids, cluster, maxCenter, meanCost\n",
    "\n",
    "#this function performs gonzales clustering, it assumes data points to be numpy vectors\n",
    "def lloyds(data, numClusters, centroids = None):\n",
    "\n",
    "  #if beginning clusters weren't set, pick clusters in order\n",
    "  #set the first cluster to be the first point\n",
    "\n",
    "  if centroids is None:\n",
    "    centroids = []\n",
    "    for i in range(0, numClusters):\n",
    "      centroids.append(data[i])\n",
    "  \n",
    "  #set the cluster mapping of all to be cluster 1\n",
    "  cluster = []\n",
    "  for i in range(len(data)):\n",
    "    cluster.append(0)\n",
    "\n",
    "  #define the number of points changed to be 1\n",
    "  numChanged = 1\n",
    "\n",
    "  #while numPoints changed > 0:\n",
    "  while numChanged > 0:\n",
    "\n",
    "    #set numChanged = 0\n",
    "    numChanged = 0\n",
    "\n",
    "    #compute the new clostest cluster, if its different, increase numChanged\n",
    "    for i in range(len(data)):\n",
    "      m = 1000000000000000000000000\n",
    "      newClust = 0\n",
    "      for j in range(len(centroids)):\n",
    "        dist = np.linalg.norm(data[i] - centroids[j])\n",
    "        if dist < m:\n",
    "          m = dist\n",
    "          newClust = j\n",
    "      if cluster[i] != newClust:\n",
    "        numChanged = numChanged+1\n",
    "      cluster[i] = newClust\n",
    "\n",
    "    #compute a new cluster value as the average of all the points\n",
    "    for j in range(len(centroids)):\n",
    "      sumPoint = np.zeros(len(data[0]))\n",
    "      numPoints =0\n",
    "      for i in range(len(data)):\n",
    "        if cluster[i] == j:\n",
    "          numPoints = numPoints+1\n",
    "          sumPoint = sumPoint+data[i]\n",
    "\n",
    "      meanPoint = sumPoint/numPoints\n",
    "      centroids[j] = meanPoint\n",
    "\n",
    "\n",
    "  #compute 3 means cost\n",
    "  #and compute  the 3 means cost\n",
    "  sum = 0\n",
    "  for i in range(len(data)):\n",
    "    dist = np.linalg.norm(data[i] - centroids[cluster[i]])\n",
    "    sum = sum + (dist * dist)\n",
    "\n",
    "\n",
    "  meanCost = sum / len(data)\n",
    "\n",
    "  #return \n",
    "  return centroids, cluster, meanCost\n",
    "\n",
    "#this functions will grphs the clusters, given the, cluster mapping and array of numpy vectors\n",
    "def graphCluster(data, clusters):\n",
    "\n",
    "  #intialize a plot\n",
    "  clusterColors = ['Red', \"Blue\", \"Green\", \"Yellow\", \"Purple\", \"Orange\", \"Teal\", \"Pink\", \"Black\", \"Grey\", \"Brown\"]\n",
    "  xs = []\n",
    "  ys = []\n",
    "  pointColors = []\n",
    "  for i in range(len(data)):\n",
    "    xs.append(data[i][0])\n",
    "    ys.append(data[i][1])\n",
    "    pointColors.append(clusterColors[clusters[i]])\n",
    "\n",
    "  plt.scatter(xs, ys, c= pointColors)\n",
    "    \n",
    "  plt.title('Scatter Plot of lloyds Clusters with gonazels starting clusters')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify anomalies, we should attempt some forms of clustering on this data. We can cluster based on emissions and deforestation, and validate the efficacy by using surface value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the datasets into arrays of numpy arrays\n",
    "\n",
    "#run and graph clusters\n",
    "\n",
    "#validate clusters with surface values. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
