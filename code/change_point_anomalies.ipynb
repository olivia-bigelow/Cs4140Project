{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e76af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d8244b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Southern Asia' 'Northern Europe' 'Southern Europe' 'Northern Africa'\n",
      " 'Polynesia' 'Sub-Saharan Africa' 'Latin America and the Caribbean' nan\n",
      " 'Western Asia' 'Australia and New Zealand' 'Western Europe'\n",
      " 'Eastern Europe' 'Northern America' 'South-eastern Asia' 'Eastern Asia'\n",
      " 'Melanesia' 'Micronesia' 'Central Asia']\n"
     ]
    }
   ],
   "source": [
    "with open('../data/continents2.csv') as f:\n",
    "  regions = pd.read_csv(f)\n",
    "  print(regions['sub-region'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e834f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterByRegion(Data, MeanAggregation=True):\n",
    "  \"\"\"\n",
    "    Clusters the Dataset by region applying the sub regions found in the continents2 dataset\n",
    "    \n",
    "    Parameters:\n",
    "        x (pd.DataFrame) the dataset to be grouped\n",
    "        MeanAggregation (boolean) if true, takes the mean of Data's Features, if false, takes the sum of the features\n",
    "    \n",
    "    Returns:\n",
    "        grouped (pd.DataFrame) a grouped dataframe \n",
    "        if the continents data cannot be openned a string is returned\n",
    "    \"\"\"\n",
    "  with open('../data/continents2.csv') as f:\n",
    "    regions = pd.read_csv(f)\n",
    "    regions = regions[['alpha-3', 'sub-region']]\n",
    "\n",
    "    merge = Data.merge(regions, how = \"left\", left_on = 'country_code', right_on ='alpha-3' )\n",
    "    merge.drop(['country_code', 'country_name', \"alpha-3\"], axis=1, inplace=True)\n",
    "\n",
    "    if MeanAggregation:\n",
    "      grouped = merge.groupby(['sub-region', 'year']).agg('mean')\n",
    "    else:\n",
    "      grouped = merge.groupby(['sub-region', 'year']).agg('sum')\n",
    "  \n",
    "  return \"FILE COULD NOT BE OPENNED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3837033",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('alpha-3', 'sub-region')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('alpha-3', 'sub-region')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m../data/co2_emissions.csv\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(f)\n\u001b[0;32m----> 3\u001b[0m     df \u001b[39m=\u001b[39m clusterByRegion(data, \u001b[39mFalse\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m, in \u001b[0;36mclusterByRegion\u001b[0;34m(Data, MeanAggregation)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m../data/continents2.csv\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     14\u001b[0m   regions \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(f)\n\u001b[0;32m---> 15\u001b[0m   regions \u001b[39m=\u001b[39m regions[\u001b[39m'\u001b[39;49m\u001b[39malpha-3\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msub-region\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     17\u001b[0m   merge \u001b[39m=\u001b[39m Data\u001b[39m.\u001b[39mmerge(regions, how \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m, left_on \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcountry_code\u001b[39m\u001b[39m'\u001b[39m, right_on \u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39malpha-3\u001b[39m\u001b[39m'\u001b[39m )\n\u001b[1;32m     18\u001b[0m   merge\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mcountry_code\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcountry_name\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39malpha-3\u001b[39m\u001b[39m\"\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: ('alpha-3', 'sub-region')"
     ]
    }
   ],
   "source": [
    "with open('../data/co2_emissions.csv') as f:\n",
    "    data = pd.read_csv(f)\n",
    "    df = clusterByRegion(data, False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17f71d0",
   "metadata": {},
   "source": [
    "Group the data by `country_code`, sort each group by year, compute the anomaly using the LLR function, run permutation testing, filter for significant anomalies (p-value < 0.05), and finally sort the results by LLR in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1966de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_llr(x):\n",
    "    \"\"\"\n",
    "    Compute the maximum log-likelihood ratio (LLR) for a change point in a 1D array x.\n",
    "    Returns the best LLR score, the index of the best change point, and the anomalous subset S*.\n",
    "    \n",
    "    Parameters:\n",
    "        x (np.array): Array of numeric data.\n",
    "    \n",
    "    Returns:\n",
    "        best_llr (float): Maximum improvement in fit (LLR score).\n",
    "        best_t (int): Index of the best change point.\n",
    "        best_subset (np.array): The anomalous subset, i.e. x[:best_t].\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    if n < 2:\n",
    "        return None, None, None\n",
    "\n",
    "    mean_all = np.mean(x)\n",
    "    sse_all = np.sum((x - mean_all) ** 2)\n",
    "\n",
    "    best_llr = -np.inf\n",
    "    best_t = None\n",
    "    best_subset = None\n",
    "\n",
    "    for t in range(1, n):\n",
    "        seg1 = x[:t]\n",
    "        seg2 = x[t:]\n",
    "        mean1 = np.mean(seg1)\n",
    "        mean2 = np.mean(seg2)\n",
    "        sse1 = np.sum((seg1 - mean1) ** 2)\n",
    "        sse2 = np.sum((seg2 - mean2) ** 2)\n",
    "        sse_split = sse1 + sse2\n",
    "        llr = sse_all - sse_split\n",
    "        if llr > best_llr:\n",
    "            best_llr = llr\n",
    "            best_t = t\n",
    "            best_subset = x[:t]\n",
    "\n",
    "    return best_llr, best_t, best_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d0f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# Process the data on a per-country basis using 'country_code'\n",
    "for country, group in df.groupby('sub-region'):\n",
    "    group_sorted = group.sort_values('year')\n",
    "    emissions = group_sorted['value'].values\n",
    "    if len(emissions) < 2:\n",
    "        continue\n",
    "\n",
    "    observed_llr, best_index, anomalous_subset = compute_llr(emissions)\n",
    "    \n",
    "    # Extract all years corresponding to the anomalous subset\n",
    "    anomalous_years = group_sorted['year'].iloc[:best_index].tolist()\n",
    "    best_year = group_sorted.iloc[best_index]['year']\n",
    "\n",
    "    num_permutations = 1000\n",
    "    llr_permutations = np.zeros(num_permutations)\n",
    "    for i in range(num_permutations):\n",
    "        emissions_perm = np.random.permutation(emissions)\n",
    "        llr_perm, _, _ = compute_llr(emissions_perm)\n",
    "        llr_permutations[i] = llr_perm if llr_perm is not None else -np.inf\n",
    "\n",
    "    p_value = np.mean(llr_permutations >= observed_llr)\n",
    "\n",
    "    results[country] = {\n",
    "        'observed_llr': observed_llr,\n",
    "        'best_change_point_index': best_index,\n",
    "        'best_year': best_year,\n",
    "        'anomalous_years': anomalous_years,\n",
    "        'p_value': p_value,\n",
    "        'n_points': len(emissions)\n",
    "    }\n",
    "\n",
    "# Filter and sort results: only include countries with a p-value < 0.05, sorted by LLR descending.\n",
    "filtered_sorted_results = sorted(\n",
    "    [(country, res) for country, res in results.items() if res['p_value'] < 0.05],\n",
    "    key=lambda item: item[1]['observed_llr'], reverse=True\n",
    ")\n",
    "\n",
    "# Display the filtered and sorted results per country\n",
    "for country, res in filtered_sorted_results:\n",
    "    print(f\"\\nCountry: {country}\")\n",
    "    print(f\"  Observed LLR: {res['observed_llr']}\")\n",
    "    print(f\"  Best Change Point Index: {res['best_change_point_index']} (Year: {res['best_year']})\")\n",
    "    print(f\"  Anomalous Years: {res['anomalous_years']}\")\n",
    "    print(f\"  p-value from permutation testing: {res['p_value']}\")\n",
    "    print(f\"  Number of Data Points: {res['n_points']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
